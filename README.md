# DLAVM-LLM

### SUSTech AoShen

DeepLearning Accelerator LLM Compiler，本项目为大模型编译器，参考TVM开发。

本项目当前主要功能为将使用编译器IR预编写的大模型计算图编译优化为可直接部署在加速器上的硬件源码，预期实现End-to-End。

## Usage

项目基于Python开发，无复杂依赖，只需编译一个动态库即可（仅Linux下需要，windows已提供dll），操作如下：

```shell
cd dlavm/clib
make
```

项目源码以python-package的形式开发，命名为dlavm。由于暂未完成，并未采用setup.py等方式加载，需要添加环境变量或者使用软链接将dlavm放置在script中方便跳转查看。


项目入口在script文件夹下，默认生成地址为./output，运行前手动创建output即可。其主要有两个main入口，其余主要作为测试或者新功能开发所使用。两个main分别对应两种代码生成方式，具体情况如下：

1. main.py

此入口为最新版本的代码生成方案，其特点为每个算子由lower IR编写tasks进行编译生成，在代码中体现为使用dlavm.backend进行编译生成。

然而，此项目并未经过严格测试，生成情况需要测试过后才能确定，并且特定的debug方案以及可视化暂未支持，待添加。

文档可参照doc/DLAVM_Compiler.md进行学习和开发。

2. codegen_main.py

此入口为之前代码生成方案，由于其生成方式比较死板，难以进行for循环等具有Region的代码的生成，所以暂时被弃用。

由于此不支持for代码的生成，所以其驱动代码（tasks）或许较旧，不一定可靠，但是其额外功能较为全面，可以生成prototxt模型结构图，以供参照。

文档可参照doc/Old_Compiler.md

---

对于此项目的学习，可以主要参考script/learn.py以及dlavm/llm目录下release的大模型计算图，并且doc/DLAVM_Compiler中详细介绍了script/learn.py的实现情况。

## 文件概览

+ backup: 一些模型部署的备份文件，由于版本迭代，仅作参考

+ compare: 指令对比测试的环境目录，可做参考

+ dlavm: 编译器的核心package

+ doc: 编译器的相关文档，根据编写时间顺序分别为CIR -> Old -> DLAVM，其中Old和DLAVM作为主要参考

+ images: 文档中的图片内容

+ script: 项目的主函数目录，以及学习文件相关的目录，仅为了较好的进行管理

## 项目说明

本项目作为大模型编译器，具有动态参数控制、内存最小化、For循环生成以及AUX等多种优化能力。然而在实际开发中，不可避免地出现新功能实现的需求，以此产生了以下为主的问题：

由于硬件版本迭代速度太快，所以其中包含了许多已经废弃的代码或硬件版本。比较好的消息是，本项目通过版本管理的方式，将他们合并在了一起且不会相互影响。

由于缺少模型前端的导入，所以当前开发方式为手动编写大模型的计算图IR代码，再进行编译。

除此之外，本项目对常量折叠以及数据排布的预处理缺少核心实现，后续期待补充与合并。

## 模型部署

对于整个模型部署而言，此项目所做的模型编译仅为模型部署的一个核心部分，其他工作依然需要一些介绍，下述将从量化模型出发以介绍完整的模型部署以及对应的代码模板（由于版本迭代，代码不一定可用，仅作学习参考）。

模型部署需要三个部分的内容：计算图（计算流程）、权重（硬件Layout预处理）以及算子。本项目主要完成了（当前）对计算图和算子的实现，后续需要完善对权重的处理。然而，对于大语言模型，由于其推理情况的特殊，在此基础上依然需要做额外的处理，主要包括以下内容：

1. 硬件算子中无法直接从模型中导出的权重的生成，主要包括PosEmb算子和所有的激活函数。PosEmb的权重参数较为固定且可以在pytorch中通过在forward函数内添加保存函数进行实现，对于激活函数，可参考backup/act_data.py的实现

2. Embedding算子的计算是在CPU上进行的，通常来说，为了加快对Embedding权重的读取、减小内存开销，会将Embedding权重分割为16份，通过代码backup/bin_split.cc实现

3. tokenizer模型为语言模型特有的模型，作为计算模型的输入。因此，需要准备当前大模型的分词模型但无需额外处理

上述所有模型部署文件准备就绪后，可以进行实际部署测试。部署平台由socket搭建，分为C++ Server（FPGA Host端）和Python Client（PC端），参考backup/demo0427代码，按位置放置权重并分别加载编译代码和分词模型即可。


